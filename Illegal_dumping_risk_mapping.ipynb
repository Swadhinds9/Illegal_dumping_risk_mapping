{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44c9a40e-caf3-413b-ba62-556a198065c5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### IDS RISK MAPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a19d640e-0acf-47a6-a684-77fc708cf940",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported Successfully\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GroupKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy.interpolate import PchipInterpolator\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.base import clone\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Imported Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc869745-cf59-44d0-bde2-f02a7aa6c7da",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid cells loaded: 454,558\n",
      "Grid CRS: EPSG:32645\n",
      "Study Area Loaded\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Load Study Area\n",
    "\n",
    "grid_path = \"D:/MURP/Thesis/Thesis/New Analysis/FN.shp\"\n",
    "grid = gpd.read_file(grid_path)\n",
    "print(f\"Grid cells loaded: {len(grid):,}\")\n",
    "print(f\"Grid CRS: {grid.crs}\")\n",
    "\n",
    "centroids = grid.geometry.centroid\n",
    "grid['centroid_x'] = centroids.x\n",
    "grid['centroid_y'] = centroids.y\n",
    "\n",
    "print(\"Study Area Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4969b381-af96-4ffc-bd20-f33011817510",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted: population_density\n",
      "  Extracted: dem\n",
      "  Extracted: wgr\n",
      "  Extracted: poverty_index\n",
      "  Extracted: lst\n",
      "  Extracted: drainage\n",
      "  Extracted: buildings\n",
      "  Extracted: railine\n",
      "  Extracted: road\n",
      "  Extracted: road_intersection\n",
      "  Extracted: waterbody\n",
      "  Extracted: sts\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Extract Raster Values\n",
    "\n",
    "raster_paths = {\n",
    "    'population_density': \"D:/MURP/IDS Review/Raster Data/Population_Density.tif\",\n",
    "    'dem': \"D:/MURP/IDS Review/Raster Data/DEM.tif\",\n",
    "    'wgr': \"D:/MURP/IDS Review/Raster Data/WGR.tif\",\n",
    "    'poverty_index': \"D:/MURP/IDS Review/Raster Data/Poverty_Index.tif\",\n",
    "    'lst': \"D:/MURP/IDS Review/Raster Data/LST.tif\",\n",
    "    'drainage': \"D:/MURP/IDS Review/Raster Data/Drain.tif\",\n",
    "    'buildings': \"D:/MURP/IDS Review/Raster Data/Building.tif\",\n",
    "    'railine': \"D:/MURP/IDS Review/Raster Data/Rail.tif\",\n",
    "    'road': \"D:/MURP/IDS Review/Raster Data/Road.tif\",\n",
    "    'road_intersection': \"D:/MURP/IDS Review/Raster Data/Intersection.tif\",\n",
    "    'waterbody': \"D:/MURP/IDS Review/Raster Data/Waterbody.tif\",\n",
    "    'sts': \"D:/MURP/IDS Review/Raster Data/STS.tif\"\n",
    "}\n",
    "\n",
    "NODATA_SENTINEL = -3.4028234663852886e+38\n",
    "\n",
    "for name, path in raster_paths.items():\n",
    "    with rasterio.open(path) as src:\n",
    "        raster_crs = src.crs\n",
    "        \n",
    "        if grid.crs != raster_crs:\n",
    "            grid_proj = grid.to_crs(raster_crs)\n",
    "            pts = grid_proj.geometry.centroid\n",
    "        else:\n",
    "            pts = centroids\n",
    "        \n",
    "        coords = [(pt.x, pt.y) for pt in pts]\n",
    "        vals = [v[0] for v in src.sample(coords)]\n",
    "        \n",
    "        vals = [\n",
    "            np.nan if (v == src.nodata or v == NODATA_SENTINEL or np.isclose(v, NODATA_SENTINEL))\n",
    "            else v for v in vals\n",
    "        ]\n",
    "        \n",
    "        grid[name] = vals\n",
    "    \n",
    "    print(f\"  Extracted: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45a9d79c-bf8a-48f8-926f-dd2530cedb54",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before imputation: 58,424\n",
      "  Spatially imputed: 57,441 values\n",
      "  Filled remaining with column means\n",
      "Missing values after imputation: 0\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: Imputation of Missing Values\n",
    "\n",
    "data = grid.drop(columns=['geometry']).copy()\n",
    "data['grid_idx'] = grid.index\n",
    "\n",
    "missing_before = data.isnull().sum().sum()\n",
    "print(f\"Missing values before imputation: {missing_before:,}\")\n",
    "\n",
    "if missing_before > 0:\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cols_to_impute = [col for col in numeric_cols if data[col].isnull().any() and col != 'grid_idx']\n",
    "    \n",
    "    idx_to_row = {grid_idx: i for i, grid_idx in enumerate(data['grid_idx'])}\n",
    "    \n",
    "    imputed_count = 0\n",
    "    for row_idx, row in data.iterrows():\n",
    "        if row.isnull().any():\n",
    "            grid_idx = row['grid_idx']\n",
    "            cell_geom = grid.loc[grid_idx, 'geometry']\n",
    "            neighbors = grid[grid.geometry.touches(cell_geom)]\n",
    "            neighbor_grid_indices = neighbors.index.tolist()\n",
    "            neighbor_data_indices = [idx_to_row[gi] for gi in neighbor_grid_indices if gi in idx_to_row]\n",
    "            \n",
    "            for col in cols_to_impute:\n",
    "                if pd.isnull(row[col]):\n",
    "                    neighbor_values = data.iloc[neighbor_data_indices][col].dropna()\n",
    "                    if len(neighbor_values) > 0:\n",
    "                        data.at[row_idx, col] = neighbor_values.mean()\n",
    "                        imputed_count += 1\n",
    "    \n",
    "    print(f\"  Spatially imputed: {imputed_count:,} values\")\n",
    "    \n",
    "    remaining_missing = data.isnull().sum().sum()\n",
    "    if remaining_missing > 0:\n",
    "        for col in cols_to_impute:\n",
    "            if data[col].isnull().any():\n",
    "                data[col].fillna(data[col].mean(), inplace=True)\n",
    "        print(f\"  Filled remaining with column means\")\n",
    "\n",
    "print(f\"Missing values after imputation: {data.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00792b7b-c7a7-4aa2-aed6-9d75f546d852",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive samples (P): 342\n",
      "Unlabeled samples (U): 454,216\n",
      "Class imbalance ratio (U:P): 1328.1:1\n",
      "\n",
      "Positive split (no leakage):\n",
      "  Training positives: 239\n",
      "  Test positives: 103\n",
      "\n",
      "Test set composition:\n",
      "  Test positives: 103\n",
      "  Test unlabeled: 103\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: Data Partitioning\n",
    "\n",
    "data['label'] = (data['Status'] == 'Positive').astype(int)\n",
    "\n",
    "P_indices = data[data['label'] == 1].index.tolist()\n",
    "U_indices = data[data['label'] == 0].index.tolist()\n",
    "\n",
    "n_P = len(P_indices)\n",
    "n_U = len(U_indices)\n",
    "\n",
    "print(f\"Positive samples (P): {n_P:,}\")\n",
    "print(f\"Unlabeled samples (U): {n_U:,}\")\n",
    "print(f\"Class imbalance ratio (U:P): {n_U/n_P:.1f}:1\")\n",
    "\n",
    "if n_P < 10:\n",
    "    raise ValueError(f\"Insufficient positive samples: {n_P}\")\n",
    "\n",
    "P_train_indices, P_test_indices = train_test_split(\n",
    "    P_indices, test_size=0.3, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "n_P_train = len(P_train_indices)\n",
    "n_P_test = len(P_test_indices)\n",
    "\n",
    "print(f\"\\nPositive split (no leakage):\")\n",
    "print(f\"  Training positives: {n_P_train}\")\n",
    "print(f\"  Test positives: {n_P_test}\")\n",
    "\n",
    "U_available_for_test = list(set(U_indices) - set(P_train_indices) - set(P_test_indices))\n",
    "U_test_indices = np.random.choice(U_available_for_test, size=n_P_test, replace=False).tolist()\n",
    "\n",
    "print(f\"\\nTest set composition:\")\n",
    "print(f\"  Test positives: {n_P_test}\")\n",
    "print(f\"  Test unlabeled: {len(U_test_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6216be4b-2bde-4207-a4cc-60b64ad0ef8a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictor features: 12\n",
      "  Distance-based: 7\n",
      "  Non-distance: 5\n"
     ]
    }
   ],
   "source": [
    "# STEP 5: Predictor Features\n",
    "\n",
    "exclude_cols = ['Status', 'label', 'ID', 'grid_idx', 'FID', 'centroid_x', 'centroid_y']\n",
    "predictor_cols = [col for col in data.columns if col not in exclude_cols]\n",
    "\n",
    "distance_based_features = [\n",
    "    'road', 'railine', 'drainage', 'road_intersection', \n",
    "    'buildings', 'sts', 'waterbody'\n",
    "]\n",
    "\n",
    "non_distance_features = [col for col in predictor_cols if col not in distance_based_features]\n",
    "\n",
    "print(f\"Total predictor features: {len(predictor_cols)}\")\n",
    "print(f\"  Distance-based: {len(distance_based_features)}\")\n",
    "print(f\"  Non-distance: {len(non_distance_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8c727c3-586a-47af-a1bb-2e5ac7603350",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature transformations:\n",
      "  Tree models: raw distances\n",
      "  Linear/Kernel-based: log-transformed distances\n",
      "Feature matrix: (454558, 12)\n"
     ]
    }
   ],
   "source": [
    "# STEP 6: Predictor for Tree vs Linear/Kernel-Based Models\n",
    "\n",
    "data_tree = data[predictor_cols].copy()\n",
    "\n",
    "data_linear = data[predictor_cols].copy()\n",
    "for col in distance_based_features:\n",
    "    data_linear[col] = np.log1p(data_linear[col])\n",
    "\n",
    "print(\"Feature transformations:\")\n",
    "print(\"  Tree models: raw distances\")\n",
    "print(\"  Linear/Kernel-based: log-transformed distances\")\n",
    "\n",
    "scaler_tree = StandardScaler()\n",
    "X_tree_full = scaler_tree.fit_transform(data_tree.values)\n",
    "\n",
    "scaler_linear = StandardScaler()\n",
    "X_linear_full = scaler_linear.fit_transform(data_linear.values)\n",
    "\n",
    "y_full = data['label'].values\n",
    "\n",
    "print(f\"Feature matrix: {X_tree_full.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "333e60ab-b31e-4ab8-9b9f-2a97ffa8f95d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VIF values:\n",
      "               Feature       VIF\n",
      "4                  lst  2.736554\n",
      "2                  wgr  2.634594\n",
      "3        poverty_index  2.489704\n",
      "8                 road  2.265788\n",
      "9    road_intersection  2.248761\n",
      "5             drainage  1.980847\n",
      "11                 sts  1.563864\n",
      "0   population_density  1.553409\n",
      "6            buildings  1.552092\n",
      "1                  dem  1.386708\n",
      "7              railine  1.334183\n",
      "10           waterbody  1.069423\n"
     ]
    }
   ],
   "source": [
    "# STEP 7: VIF Check\n",
    "\n",
    "X_linear_df = pd.DataFrame(X_linear_full, columns=predictor_cols)\n",
    "\n",
    "def compute_vif(df):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data['Feature'] = df.columns\n",
    "    vif_data['VIF'] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "    return vif_data.sort_values('VIF', ascending=False)\n",
    "\n",
    "vif = compute_vif(X_linear_df)\n",
    "print(\"\\nVIF values:\")\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51dbb54d-d5f6-4482-ac0d-f696954eb7e3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 5 spatial blocks\n",
      "  Block 0: 111,250 cells (24.5%)\n",
      "  Block 1: 70,505 cells (15.5%)\n",
      "  Block 2: 74,662 cells (16.4%)\n",
      "  Block 3: 99,911 cells (22.0%)\n",
      "  Block 4: 98,230 cells (21.6%)\n",
      "\n",
      "Positive distribution across spatial blocks:\n",
      "  Block 0: 83 positives\n",
      "  Block 1: 52 positives\n",
      "  Block 2: 29 positives\n",
      "  Block 3: 76 positives\n",
      "  Block 4: 102 positives\n"
     ]
    }
   ],
   "source": [
    "# STEP 8: Spatial Blocks\n",
    "\n",
    "coords = data[['centroid_x', 'centroid_y']].values\n",
    "n_blocks = 5\n",
    "kmeans = KMeans(n_clusters=n_blocks, random_state=42, n_init=10)\n",
    "spatial_blocks = kmeans.fit_predict(coords)\n",
    "\n",
    "data['spatial_block'] = spatial_blocks\n",
    "\n",
    "print(f\"Created {n_blocks} spatial blocks\")\n",
    "for block_id in range(n_blocks):\n",
    "    block_size = np.sum(spatial_blocks == block_id)\n",
    "    print(f\"  Block {block_id}: {block_size:,} cells ({100*block_size/len(data):.1f}%)\")\n",
    "\n",
    "print(\"\\nPositive distribution across spatial blocks:\")\n",
    "for block_id in range(n_blocks):\n",
    "    block_mask = spatial_blocks == block_id\n",
    "    n_positives_in_block = np.sum((data['label'] == 1) & block_mask)\n",
    "    print(f\"  Block {block_id}: {n_positives_in_block} positives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db0ad112-e9e7-42ff-88e7-fb723bfff608",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset size: 478\n",
      "  Positives: 239\n",
      "  Unlabeled: 239\n"
     ]
    }
   ],
   "source": [
    "# STEP 9: Balanced Training Dataset\n",
    "\n",
    "n_sample_U = min(n_P_train, len([i for i in U_indices if i not in P_test_indices and i not in U_test_indices]))\n",
    "U_available_for_tuning = [i for i in U_indices if i not in P_test_indices and i not in U_test_indices]\n",
    "sampled_U_tuning = np.random.choice(U_available_for_tuning, size=n_sample_U, replace=False).tolist()\n",
    "\n",
    "balanced_indices = P_train_indices + sampled_U_tuning\n",
    "balanced_labels = [1]*n_P_train + [0]*n_sample_U\n",
    "\n",
    "X_tree_balanced = X_tree_full[balanced_indices]\n",
    "X_linear_balanced = X_linear_full[balanced_indices]\n",
    "\n",
    "y_balanced = np.array(balanced_labels)\n",
    "groups_balanced = spatial_blocks[balanced_indices]\n",
    "\n",
    "print(f\"Balanced dataset size: {len(balanced_indices):,}\")\n",
    "print(f\"  Positives: {n_P_train}\")\n",
    "print(f\"  Unlabeled: {n_sample_U}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d277ad5a-730b-41f3-9558-4b8d2788f67c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuning RandomForest...\n",
      "  Best CV AUC: 0.8828 ±0.0272\n",
      "\n",
      "Tuning XGBoost...\n",
      "  Best CV AUC: 0.8769 ±0.0313\n",
      "\n",
      "Tuning LightGBM...\n",
      "  Best CV AUC: 0.8670 ±0.0271\n"
     ]
    }
   ],
   "source": [
    "# STEP 10: Hyperparameter tuning - Tree-Based Models\n",
    "\n",
    "tree_models = {\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(random_state=42, n_jobs=-1, class_weight='balanced'),\n",
    "        'params': {\n",
    "            'n_estimators': [200, 500],\n",
    "            'max_depth': [10, 20, None],\n",
    "            'min_samples_split': [5, 10],\n",
    "            'min_samples_leaf': [2, 4],\n",
    "            'max_features': ['sqrt', 'log2']\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': XGBClassifier(random_state=42, n_jobs=-1, tree_method='hist', eval_metric='logloss'),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 300],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'colsample_bytree': [0.8, 1.0],\n",
    "            'min_child_weight': [1, 3]\n",
    "        }\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'model': LGBMClassifier(random_state=42, n_jobs=-1, class_weight='balanced', verbosity=-1),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 300],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'num_leaves': [15, 31, 63],\n",
    "            'min_child_samples': [10, 20],\n",
    "            'subsample': [0.8, 1.0]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "best_tree_models = {}\n",
    "tree_tuning_results = []\n",
    "\n",
    "n_splits = min(5, len(np.unique(groups_balanced)))\n",
    "cv_splitter = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "for model_name, config in tree_models.items():\n",
    "    print(f\"\\nTuning {model_name}...\")\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=config['model'],\n",
    "        param_grid=config['params'],\n",
    "        cv=cv_splitter,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_tree_balanced, y_balanced, groups=groups_balanced)\n",
    "    \n",
    "    best_tree_models[model_name] = grid_search.best_estimator_\n",
    "    \n",
    "    tree_tuning_results.append({\n",
    "        'Model': model_name,\n",
    "        'Best_Params': str(grid_search.best_params_),\n",
    "        'CV_AUC_Mean': grid_search.best_score_,\n",
    "        'CV_AUC_Std': grid_search.cv_results_['std_test_score'][grid_search.best_index_]\n",
    "    })\n",
    "    \n",
    "    print(f\"  Best CV AUC: {grid_search.best_score_:.4f} ±{grid_search.cv_results_['std_test_score'][grid_search.best_index_]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08f62156-14bb-4c1c-8159-747c586694d7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuning LogisticReg...\n",
      "  Best CV AUC: 0.8755 ±0.0124\n",
      "\n",
      "Tuning SVM...\n",
      "  Best CV AUC: 0.8775 ±0.0221\n",
      "\n",
      "Tuning MLP...\n",
      "  Best CV AUC: 0.8742 ±0.0169\n",
      "\n",
      "Tuning KNN...\n",
      "  Best CV AUC: 0.8625 ±0.0179\n",
      "       Model  CV_AUC_Mean  CV_AUC_Std\n",
      "RandomForest     0.882828    0.027185\n",
      "         SVM     0.877490    0.022065\n",
      "     XGBoost     0.876874    0.031328\n",
      " LogisticReg     0.875467    0.012441\n",
      "         MLP     0.874249    0.016911\n",
      "    LightGBM     0.867036    0.027077\n",
      "         KNN     0.862473    0.017893\n"
     ]
    }
   ],
   "source": [
    "# STEP 11: Hyperparameter tuning - Linear/Kernel models\n",
    "\n",
    "X_linear_balanced_filtered = X_linear_full[balanced_indices]\n",
    "\n",
    "linear_models = {\n",
    "    'LogisticReg': {\n",
    "        'model': LogisticRegression(random_state=42, max_iter=2000, n_jobs=-1, class_weight='balanced'),\n",
    "        'params': {\n",
    "            'C': [0.01, 0.1, 1, 10],\n",
    "            'penalty': ['l2'],\n",
    "            'solver': ['lbfgs']\n",
    "        }\n",
    "    },\n",
    "    'SVM': {\n",
    "        'model': SVC(random_state=42, probability=True, class_weight='balanced', cache_size=500),\n",
    "        'params': {\n",
    "            'C': [0.1, 1, 10, 100],\n",
    "            'kernel': ['rbf', 'poly'],\n",
    "            'gamma': ['scale', 0.01, 0.1]\n",
    "        }\n",
    "    },\n",
    "    'MLP': {\n",
    "        'model': MLPClassifier(random_state=42, max_iter=1000, early_stopping=True),\n",
    "        'params': {\n",
    "            'hidden_layer_sizes': [(100,), (100, 50), (150, 75, 25)],\n",
    "            'activation': ['relu', 'tanh'],\n",
    "            'alpha': [0.0001, 0.001, 0.01],\n",
    "            'learning_rate': ['adaptive']\n",
    "        }\n",
    "    },\n",
    "    'KNN': {\n",
    "        'model': KNeighborsClassifier(n_jobs=-1),\n",
    "        'params': {\n",
    "            'n_neighbors': [5, 9, 15, 21],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'metric': ['euclidean', 'manhattan'],\n",
    "            'p': [1, 2]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "best_linear_models = {}\n",
    "linear_tuning_results = []\n",
    "\n",
    "for model_name, config in linear_models.items():\n",
    "    print(f\"\\nTuning {model_name}...\")\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=config['model'],\n",
    "        param_grid=config['params'],\n",
    "        cv=cv_splitter,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_linear_balanced_filtered, y_balanced, groups=groups_balanced)\n",
    "    \n",
    "    best_linear_models[model_name] = grid_search.best_estimator_\n",
    "    \n",
    "    linear_tuning_results.append({\n",
    "        'Model': model_name,\n",
    "        'Best_Params': str(grid_search.best_params_),\n",
    "        'CV_AUC_Mean': grid_search.best_score_,\n",
    "        'CV_AUC_Std': grid_search.cv_results_['std_test_score'][grid_search.best_index_]\n",
    "    })\n",
    "    \n",
    "    print(f\"  Best CV AUC: {grid_search.best_score_:.4f} ±{grid_search.cv_results_['std_test_score'][grid_search.best_index_]:.4f}\")\n",
    "\n",
    "best_models = {**best_tree_models, **best_linear_models}\n",
    "\n",
    "tuning_df = pd.DataFrame(tree_tuning_results + linear_tuning_results)\n",
    "tuning_df = tuning_df.sort_values('CV_AUC_Mean', ascending=False)\n",
    "\n",
    "print(tuning_df[['Model', 'CV_AUC_Mean', 'CV_AUC_Std']].to_string(index=False))\n",
    "\n",
    "tuning_df.to_csv('hyperparameter_tuning.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce57361f-da1e-46d9-9154-b3b5c0157866",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations per model: 500\n",
      "PU BAGGING: RandomForest\n",
      "  Threshold 0.55: 500/500 valid (100.0%)\n",
      "    Mean LL: 0.8233\n",
      "  Threshold 0.60: 500/500 valid (100.0%)\n",
      "    Mean LL: 0.8233\n",
      "  Threshold 0.65: 500/500 valid (100.0%)\n",
      "    Mean LL: 0.8233\n",
      "  Threshold 0.70: 500/500 valid (100.0%)\n",
      "    Mean LL: 0.8233\n",
      "\n",
      "  Selected threshold: 0.55 (500 classifiers)\n",
      "PU BAGGING: XGBoost\n",
      "  Threshold 0.55: 500/500 valid (100.0%)\n",
      "    Mean LL: 0.8232\n",
      "  Threshold 0.60: 500/500 valid (100.0%)\n",
      "    Mean LL: 0.8232\n",
      "  Threshold 0.65: 500/500 valid (100.0%)\n",
      "    Mean LL: 0.8232\n",
      "  Threshold 0.70: 500/500 valid (100.0%)\n",
      "    Mean LL: 0.8232\n",
      "\n",
      "  Selected threshold: 0.55 (500 classifiers)\n",
      "PU BAGGING: LightGBM\n",
      "  Threshold 0.55: 500/500 valid (100.0%)\n",
      "    Mean LL: 0.8161\n",
      "  Threshold 0.60: 500/500 valid (100.0%)\n",
      "    Mean LL: 0.8161\n",
      "  Threshold 0.65: 500/500 valid (100.0%)\n",
      "    Mean LL: 0.8161\n",
      "  Threshold 0.70: 500/500 valid (100.0%)\n",
      "    Mean LL: 0.8161\n",
      "\n",
      "  Selected threshold: 0.55 (500 classifiers)\n",
      "PU BAGGING: LogisticReg\n",
      "  Threshold 0.55: 500/500 valid (100.0%)\n",
      "    Mean LL: 0.8180\n",
      "  Threshold 0.60: 500/500 valid (100.0%)\n",
      "    Mean LL: 0.8180\n",
      "  Threshold 0.65: 500/500 valid (100.0%)\n",
      "    Mean LL: 0.8180\n",
      "  Threshold 0.70: 497/500 valid (99.4%)\n",
      "    Mean LL: 0.8186\n",
      "\n",
      "  Selected threshold: 0.55 (500 classifiers)\n",
      "PU BAGGING: SVM\n",
      "  Threshold 0.55: 500/500 valid (100.0%)\n",
      "    Mean LL: 0.8269\n",
      "  Threshold 0.60: 500/500 valid (100.0%)\n",
      "    Mean LL: 0.8269\n",
      "  Threshold 0.65: 500/500 valid (100.0%)\n",
      "    Mean LL: 0.8269\n",
      "  Threshold 0.70: 498/500 valid (99.6%)\n",
      "    Mean LL: 0.8273\n",
      "\n",
      "  Selected threshold: 0.55 (500 classifiers)\n",
      "PU BAGGING: MLP\n",
      "  Threshold 0.55: 500/500 valid (100.0%)\n",
      "    Mean LL: 0.8109\n",
      "  Threshold 0.60: 500/500 valid (100.0%)\n",
      "    Mean LL: 0.8109\n",
      "  Threshold 0.65: 500/500 valid (100.0%)\n",
      "    Mean LL: 0.8109\n",
      "  Threshold 0.70: 483/500 valid (96.6%)\n",
      "    Mean LL: 0.8137\n",
      "\n",
      "  Selected threshold: 0.55 (500 classifiers)\n",
      "PU BAGGING: KNN\n",
      "  Threshold 0.55: 500/500 valid (100.0%)\n",
      "    Mean LL: 0.8036\n",
      "  Threshold 0.60: 500/500 valid (100.0%)\n",
      "    Mean LL: 0.8036\n",
      "  Threshold 0.65: 500/500 valid (100.0%)\n",
      "    Mean LL: 0.8036\n",
      "  Threshold 0.70: 493/500 valid (98.6%)\n",
      "    Mean LL: 0.8050\n",
      "\n",
      "  Selected threshold: 0.55 (500 classifiers)\n",
      "PU BAGGING COMPLETE\n"
     ]
    }
   ],
   "source": [
    "# STEP 12: PU Bagging\n",
    "\n",
    "n_iterations = 500\n",
    "print(f\"Iterations per model: {n_iterations}\")\n",
    "\n",
    "sampled_U_indices_iterations = np.array([\n",
    "    np.random.choice([i for i in U_indices if i not in P_test_indices and i not in U_test_indices], \n",
    "                     size=n_P_train, replace=False)\n",
    "    for _ in range(n_iterations)\n",
    "])\n",
    "\n",
    "def calculate_ll_metric(tp, fp, fn, tn):\n",
    "    if (tp + fp) == 0 or (tp + fn) == 0:\n",
    "        return 0.0\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "reliability_thresholds = [0.55, 0.60, 0.65, 0.70]\n",
    "\n",
    "threshold_sensitivity_results = defaultdict(list)\n",
    "final_predictions_by_threshold = {}\n",
    "\n",
    "for model_name in best_models.keys():\n",
    "    print(f\"PU BAGGING: {model_name}\")\n",
    "    \n",
    "    is_tree_model = model_name in best_tree_models\n",
    "    base_model = best_models[model_name]\n",
    "    X_full_model = X_tree_full if is_tree_model else X_linear_full\n",
    "    X_balanced_model = X_tree_balanced if is_tree_model else X_linear_balanced_filtered\n",
    "    \n",
    "    for threshold in reliability_thresholds:\n",
    "        iteration_predictions = []\n",
    "        iteration_weights = []\n",
    "        valid_count = 0\n",
    "        \n",
    "        for i in range(n_iterations):\n",
    "            try:\n",
    "                Ui = sampled_U_indices_iterations[i]\n",
    "                Mi_indices = np.concatenate([P_train_indices, Ui])\n",
    "                Mi_labels = np.array([1]*n_P_train + [0]*n_P_train)\n",
    "                \n",
    "                X_Mi = X_full_model[Mi_indices]\n",
    "                y_Mi = Mi_labels\n",
    "                \n",
    "                X_train_i, X_test_i, y_train_i, y_test_i = train_test_split(\n",
    "                    X_Mi, y_Mi, test_size=0.2, random_state=i, stratify=y_Mi\n",
    "                )\n",
    "                \n",
    "                model_i = clone(base_model)\n",
    "                model_i.fit(X_train_i, y_train_i)\n",
    "                \n",
    "                y_pred_test = model_i.predict(X_test_i)\n",
    "                cm = confusion_matrix(y_test_i, y_pred_test)\n",
    "                \n",
    "                if cm.shape != (2, 2):\n",
    "                    continue\n",
    "                \n",
    "                tn, fp, fn, tp = cm.ravel()\n",
    "                ll_score = calculate_ll_metric(tp, fp, fn, tn)\n",
    "                accuracy = (tp + tn) / len(y_test_i)\n",
    "                \n",
    "                if ll_score >= threshold and accuracy >= threshold:\n",
    "                    if hasattr(model_i, 'predict_proba'):\n",
    "                        proba_full = model_i.predict_proba(X_full_model)[:, 1]\n",
    "                    else:\n",
    "                        if hasattr(model_i, 'decision_function'):\n",
    "                            decision = model_i.decision_function(X_full_model)\n",
    "                            proba_full = (decision - decision.min()) / (decision.max() - decision.min() + 1e-10)\n",
    "                        else:\n",
    "                            proba_full = model_i.predict(X_full_model).astype(float)\n",
    "                    \n",
    "                    iteration_predictions.append(proba_full)\n",
    "                    iteration_weights.append(ll_score)\n",
    "                    valid_count += 1\n",
    "            \n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if valid_count < 50:\n",
    "            print(f\"  Threshold {threshold:.2f}: Insufficient valid classifiers ({valid_count})\")\n",
    "            continue\n",
    "        \n",
    "        iteration_predictions = np.array(iteration_predictions)\n",
    "        iteration_weights = np.array(iteration_weights)\n",
    "        normalized_weights = iteration_weights / iteration_weights.sum()\n",
    "        \n",
    "        final_pred_weighted = np.average(iteration_predictions, axis=0, weights=normalized_weights)\n",
    "        final_pred_unweighted = np.mean(iteration_predictions, axis=0)\n",
    "        \n",
    "        threshold_sensitivity_results[model_name].append({\n",
    "            'threshold': threshold,\n",
    "            'valid_classifiers': valid_count,\n",
    "            'mean_ll_score': np.mean(iteration_weights),\n",
    "            'weighted_pred': final_pred_weighted,\n",
    "            'unweighted_pred': final_pred_unweighted\n",
    "        })\n",
    "        \n",
    "        print(f\"  Threshold {threshold:.2f}: {valid_count}/{n_iterations} valid ({100*valid_count/n_iterations:.1f}%)\")\n",
    "        print(f\"    Mean LL: {np.mean(iteration_weights):.4f}\")\n",
    "\n",
    "    best_threshold_idx = np.argmax([r['valid_classifiers'] for r in threshold_sensitivity_results[model_name]])\n",
    "    best_result = threshold_sensitivity_results[model_name][best_threshold_idx]\n",
    "    \n",
    "    final_predictions_by_threshold[model_name] = {\n",
    "        'threshold': best_result['threshold'],\n",
    "        'weighted': best_result['weighted_pred'],\n",
    "        'unweighted': best_result['unweighted_pred'],\n",
    "        'valid_count': best_result['valid_classifiers']\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n  Selected threshold: {best_result['threshold']:.2f} ({best_result['valid_classifiers']} classifiers)\")\n",
    "\n",
    "print(\"PU BAGGING COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8c226fb-2d1e-4578-818b-11b413254b7d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPATIAL CV SUMMARY (95% CI from empirical fold scores)\n",
      "       Model  AUC_Mean  AUC_CI_Low  AUC_CI_High  F1_Mean\n",
      "RandomForest     0.883       0.841        0.918    0.820\n",
      "         SVM     0.878       0.852        0.911    0.788\n",
      "     XGBoost     0.876       0.821        0.920    0.811\n",
      " LogisticReg     0.875       0.860        0.894    0.797\n",
      "         MLP     0.874       0.849        0.897    0.794\n",
      "    LightGBM     0.867       0.831        0.903    0.762\n",
      "         KNN     0.862       0.844        0.891    0.768\n"
     ]
    }
   ],
   "source": [
    "# STEP 13: Spatial CV\n",
    "\n",
    "cv_fold_results = defaultdict(lambda: {'auc': [], 'f1': [], 'recall': [], 'precision': []})\n",
    "\n",
    "n_splits_cv = 5\n",
    "gkf = GroupKFold(n_splits=n_splits_cv)\n",
    "\n",
    "for model_name in best_models.keys():    \n",
    "    is_tree_model = model_name in best_tree_models\n",
    "    X_cv = X_tree_balanced if is_tree_model else X_linear_balanced_filtered\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(gkf.split(X_cv, y_balanced, groups=groups_balanced), 1):\n",
    "        X_tr, X_te = X_cv[train_idx], X_cv[test_idx]\n",
    "        y_tr, y_te = y_balanced[train_idx], y_balanced[test_idx]\n",
    "        \n",
    "        model = clone(best_models[model_name])\n",
    "        model.fit(X_tr, y_tr)\n",
    "        \n",
    "        y_pred = model.predict(X_te)\n",
    "        \n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_proba = model.predict_proba(X_te)[:, 1]\n",
    "        else:\n",
    "            if hasattr(model, 'decision_function'):\n",
    "                df = model.decision_function(X_te)\n",
    "                y_proba = (df - df.min()) / (df.max() - df.min() + 1e-10)\n",
    "            else:\n",
    "                y_proba = y_pred.astype(float)\n",
    "        \n",
    "        cv_fold_results[model_name]['auc'].append(roc_auc_score(y_te, y_proba))\n",
    "        cv_fold_results[model_name]['f1'].append(f1_score(y_te, y_pred, zero_division=0))\n",
    "        cv_fold_results[model_name]['recall'].append(recall_score(y_te, y_pred, zero_division=0))\n",
    "        cv_fold_results[model_name]['precision'].append(precision_score(y_te, y_pred, zero_division=0))\n",
    "\n",
    "spatial_cv_summary = []\n",
    "for model_name in best_models.keys():\n",
    "    auc_scores = cv_fold_results[model_name]['auc']\n",
    "    f1_scores = cv_fold_results[model_name]['f1']\n",
    "    recall_scores = cv_fold_results[model_name]['recall']\n",
    "    precision_scores = cv_fold_results[model_name]['precision']\n",
    "    \n",
    "    spatial_cv_summary.append({\n",
    "        'Model': model_name,\n",
    "        'AUC_Mean': np.mean(auc_scores),\n",
    "        'AUC_CI_Low': np.percentile(auc_scores, 2.5),\n",
    "        'AUC_CI_High': np.percentile(auc_scores, 97.5),\n",
    "        'F1_Mean': np.mean(f1_scores),\n",
    "        'F1_CI_Low': np.percentile(f1_scores, 2.5),\n",
    "        'F1_CI_High': np.percentile(f1_scores, 97.5),\n",
    "        'Recall_Mean': np.mean(recall_scores),\n",
    "        'Precision_Mean': np.mean(precision_scores)\n",
    "    })\n",
    "    \n",
    "spatial_cv_df = pd.DataFrame(spatial_cv_summary)\n",
    "spatial_cv_df = spatial_cv_df.sort_values('AUC_Mean', ascending=False)\n",
    "\n",
    "print(\"SPATIAL CV SUMMARY (95% CI from empirical fold scores)\")\n",
    "print(spatial_cv_df[['Model', 'AUC_Mean', 'AUC_CI_Low', 'AUC_CI_High', 'F1_Mean']].round(3).to_string(index=False))\n",
    "\n",
    "spatial_cv_df.to_csv('spatial_cv.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59bb8fca-c8b5-47bb-b9ca-976726e79552",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best Model across weighting schemes:\n",
      "  F1=0.4, AUC=0.6: RandomForest\n",
      "  F1=0.5, AUC=0.5: RandomForest\n",
      "  F1=0.6, AUC=0.4: RandomForest\n",
      "  F1=0.7, AUC=0.3: RandomForest\n",
      "\\Best model: RandomForest\n",
      "Using weighted ensemble for RandomForest\n"
     ]
    }
   ],
   "source": [
    "# STEP 14: Model selection across differrent Weighting Scheme\n",
    "\n",
    "weighting_schemes = [0.4, 0.5, 0.6, 0.7]\n",
    "model_selection_sensitivity = []\n",
    "\n",
    "for f1_weight in weighting_schemes:\n",
    "    auc_weight = 1 - f1_weight\n",
    "    \n",
    "    for _, row in spatial_cv_df.iterrows():\n",
    "        rank_score = f1_weight * row['F1_Mean'] + auc_weight * row['AUC_Mean']\n",
    "        model_selection_sensitivity.append({\n",
    "            'Model': row['Model'],\n",
    "            'F1_Weight': f1_weight,\n",
    "            'AUC_Weight': auc_weight,\n",
    "            'Rank_Score': rank_score\n",
    "        })\n",
    "\n",
    "selection_df = pd.DataFrame(model_selection_sensitivity)\n",
    "print(\"\\n Best Model across weighting schemes:\")\n",
    "for f1_weight in weighting_schemes:\n",
    "    subset = selection_df[selection_df['F1_Weight'] == f1_weight].sort_values('Rank_Score', ascending=False)\n",
    "    best = subset.iloc[0]['Model']\n",
    "    print(f\"  F1={f1_weight:.1f}, AUC={1-f1_weight:.1f}: {best}\")\n",
    "\n",
    "best_model = selection_df.groupby('Model')['Rank_Score'].mean().idxmax()\n",
    "print(f\"\\Best model: {best_model}\")\n",
    "\n",
    "final_model_name = best_model\n",
    "final_predictions_weighted = final_predictions_by_threshold[final_model_name]['weighted']\n",
    "final_predictions_unweighted = final_predictions_by_threshold[final_model_name]['unweighted']\n",
    "\n",
    "print(f\"Using weighted ensemble for {final_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b80198bb-7f15-433b-968f-0e0b57580c15",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# STEP 15: Probability Calibration & Distribution\n",
    "\n",
    "X_test_holdout = np.vstack([\n",
    "    X_tree_full[P_test_indices] if final_model_name in best_tree_models else X_linear_full[P_test_indices],\n",
    "    X_tree_full[U_test_indices] if final_model_name in best_tree_models else X_linear_full[U_test_indices]\n",
    "])\n",
    "\n",
    "y_test_holdout = np.hstack([np.ones(n_P_test), np.zeros(len(U_test_indices))])\n",
    "\n",
    "test_indices_combined = P_test_indices + U_test_indices\n",
    "final_pred_test = final_predictions_weighted[test_indices_combined]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "prob_true, prob_pred = calibration_curve(y_test_holdout, final_pred_test, n_bins=10, strategy='uniform')\n",
    "ax1.plot(prob_pred, prob_true, marker='o', linewidth=2, label=final_model_name)\n",
    "ax1.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n",
    "ax1.set_xlabel('Predicted probability', fontsize=12)\n",
    "ax1.set_ylabel('True probability (in bin)', fontsize=12)\n",
    "ax1.set_title('Calibration Curve (Test Set)', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "ax2.hist(final_pred_test[y_test_holdout == 1], bins=20, alpha=0.6, label='Positives', color='red')\n",
    "ax2.hist(final_pred_test[y_test_holdout == 0], bins=20, alpha=0.6, label='Unlabeled', color='blue')\n",
    "ax2.set_xlabel('Predicted probability', fontsize=12)\n",
    "ax2.set_ylabel('Frequency', fontsize=12)\n",
    "ax2.set_title('Prediction Distribution', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "ax1.text(0.02, 0.95, '(a)', transform=ax1.transAxes,\n",
    "         fontsize=14, fontweight='bold', va='top')\n",
    "\n",
    "ax2.text(0.02, 0.95, '(b)', transform=ax2.transAxes,\n",
    "         fontsize=14, fontweight='bold', va='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Times New Roman'],\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 13,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    'legend.fontsize': 12\n",
    "})\n",
    "\n",
    "plt.savefig('probability_calibration_and_distribution.png', dpi=1200, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "23f280fd-24d9-4409-aa43-c47a4a42d196",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set performance (RandomForest):\n",
      "  AUC (lower bound): 0.8834\n",
      "  Recall on known positives: 0.8447\n",
      "  Apparent precision (biased by U contamination): 0.7982\n",
      "  Apparent F1 (biased by U contamination): 0.8208\n",
      "\n",
      "WARNING: Precision/F1 are underestimates due to unlabeled positives in test negatives\n"
     ]
    }
   ],
   "source": [
    "# STEP 16: Holdout test set evaluation\n",
    "\n",
    "test_auc = roc_auc_score(y_test_holdout, final_pred_test)\n",
    "test_pred_binary = (final_pred_test > 0.5).astype(int)\n",
    "\n",
    "recall_on_known_positives = recall_score(y_test_holdout, test_pred_binary)\n",
    "precision_apparent = precision_score(y_test_holdout, test_pred_binary, zero_division=0)\n",
    "f1_apparent = f1_score(y_test_holdout, test_pred_binary, zero_division=0)\n",
    "\n",
    "print(f\"\\nTest set performance ({final_model_name}):\")\n",
    "print(f\"  AUC (lower bound): {test_auc:.4f}\")\n",
    "print(f\"  Recall on known positives: {recall_on_known_positives:.4f}\")\n",
    "print(f\"  Apparent precision (biased by U contamination): {precision_apparent:.4f}\")\n",
    "print(f\"  Apparent F1 (biased by U contamination): {f1_apparent:.4f}\")\n",
    "print(f\"\\nWARNING: Precision/F1 are underestimates due to unlabeled positives in test negatives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "deb23335-c364-4eae-8c38-81bd0c56d09c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# STEP 17: Final Outputs\n",
    "data['dump_risk_prob'] = final_predictions_weighted\n",
    "\n",
    "predictions_output = data[['centroid_x', 'centroid_y', 'dump_risk_prob']].copy()\n",
    "predictions_output.columns = ['Longitude', 'Latitude', 'Risk_Probability']\n",
    "predictions_output.to_csv('final_dump_risk_predictions.csv', index=False)\n",
    "\n",
    "model_comparison = spatial_cv_df.copy()\n",
    "model_comparison['PU_Valid_Classifiers'] = [\n",
    "    final_predictions_by_threshold[m]['valid_count'] if m in final_predictions_by_threshold else 0\n",
    "    for m in model_comparison['Model']\n",
    "]\n",
    "model_comparison['PU_Threshold'] = [\n",
    "    final_predictions_by_threshold[m]['threshold'] if m in final_predictions_by_threshold else np.nan\n",
    "    for m in model_comparison['Model']\n",
    "]\n",
    "model_comparison.to_csv('model_comparison_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "260fd0e7-b35d-48f7-992a-5cd76f633c16",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved: pareto.png\n"
     ]
    }
   ],
   "source": [
    "# STEP 18: Feature Importance\n",
    "final_model = best_models[final_model_name]\n",
    "\n",
    "feature_name_mapping = {\n",
    "    'population_density': 'Population Density',\n",
    "    'poverty_index': 'Poverty Index',\n",
    "    'infrastructure_index': 'Infrastructure Index',\n",
    "    'livelihood_index': 'Livelihood Index',\n",
    "    'sts': 'STS',\n",
    "    'informal_settlement': 'Informal Settlement',\n",
    "    'road_intersection': 'Road Intersection',\n",
    "    'railine': 'Railway',\n",
    "    'road': 'Road',\n",
    "    'drainage': 'Drainage',\n",
    "    'water': 'Waterbody',\n",
    "    'buildings': 'Buildings',\n",
    "    'lst': 'LST',\n",
    "    'dem': 'DEM',\n",
    "    'wgr': 'Waste Generation Rate'\n",
    "}\n",
    "\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature_Short': feature_names_short,\n",
    "    'Feature': [feature_name_mapping.get(f, f) for f in feature_names_short],\n",
    "    'Importance': feature_importance,\n",
    "    'Percentage': feature_importance * 100\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "importance_df['Cumulative_Percentage'] = importance_df['Percentage'].cumsum()\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Times New Roman'],\n",
    "    'font.size': 14,\n",
    "    'axes.labelsize': 18,\n",
    "    'axes.titlesize': 20,\n",
    "    'xtick.labelsize': 16,\n",
    "    'ytick.labelsize': 16,\n",
    "    'legend.fontsize': 14,\n",
    "    'figure.dpi': 1200\n",
    "})\n",
    "\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "x_pos = np.arange(len(importance_df))\n",
    "\n",
    "\n",
    "bars = ax1.bar(\n",
    "    x_pos,\n",
    "    importance_df['Percentage'],\n",
    "    color='#d9d9d9',          # ash gray\n",
    "    edgecolor='black',\n",
    "    linewidth=0.8,\n",
    "    width=0.7,\n",
    "    alpha=0.95,\n",
    "    label='Individual Importance'\n",
    ")\n",
    "\n",
    "\n",
    "for idx, pct in zip(x_pos, importance_df['Percentage']):\n",
    "    ax1.text(\n",
    "        idx, pct / 2,\n",
    "        f'{pct:.1f}%',\n",
    "        ha='center',\n",
    "        va='center',\n",
    "        fontsize=12,\n",
    "        fontweight='bold'\n",
    "    )\n",
    "\n",
    "ax1.set_xlabel('Predictors', fontweight='bold')\n",
    "ax1.set_ylabel('Importance (%)', fontweight='bold')\n",
    "ax1.set_ylim(0, importance_df['Percentage'].max() * 1.15)\n",
    "ax1.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "ax1.set_axisbelow(True)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(\n",
    "    x_pos,\n",
    "    importance_df['Cumulative_Percentage'],\n",
    "    color='#b22222',\n",
    "    marker='o',\n",
    "    linewidth=2.5,\n",
    "    markersize=6,\n",
    "    markerfacecolor='#b22222',\n",
    "    markeredgecolor='black',\n",
    "    label='Cumulative Importance'\n",
    ")\n",
    "\n",
    "for idx, cum in zip(x_pos, importance_df['Cumulative_Percentage']):\n",
    "    ax2.text(\n",
    "        idx, cum + 2,\n",
    "        f'{cum:.1f}%',\n",
    "        ha='center',\n",
    "        va='bottom',\n",
    "        fontsize=11,\n",
    "        fontweight='bold',\n",
    "        color='#b22222'\n",
    "    )\n",
    "\n",
    "ax2.set_ylabel('Cumulative Importance (%)', fontweight='bold')\n",
    "ax2.set_ylim(0, 105)\n",
    "ax2.axhline(80, color='#2ca25f', linestyle='--', linewidth=1.5, alpha=0.7, label='80% threshold')\n",
    "ax2.axhline(95, color='#f16913', linestyle='--', linewidth=1.5, alpha=0.7, label='95% threshold')\n",
    "\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(\n",
    "    importance_df['Feature'],\n",
    "    rotation=90,\n",
    "    ha='right'\n",
    ")\n",
    "\n",
    "legend1 = ax1.legend(\n",
    "    loc='center left',\n",
    "    bbox_to_anchor=(0.78, 0.55),\n",
    "    framealpha=0.9\n",
    ")\n",
    "\n",
    "legend2 = ax2.legend(\n",
    "    loc='center left',\n",
    "    bbox_to_anchor=(0.78, 0.40),\n",
    "    framealpha=0.9\n",
    ")\n",
    "\n",
    "ax1.add_artist(legend1)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    'pareto_chart.png',\n",
    "    dpi=1200,\n",
    "    bbox_inches='tight',\n",
    "    facecolor='white'\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved: pareto.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6cfe8243-d661-4683-a397-bbe7bb68602f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved: ROC.png\n"
     ]
    }
   ],
   "source": [
    "# STEP 19 : ROC\n",
    "\n",
    "required_vars = [\n",
    "    'final_predictions_by_threshold',\n",
    "    'P_test_indices',\n",
    "    'U_test_indices'\n",
    "]\n",
    "\n",
    "for v in required_vars:\n",
    "    if v not in globals():\n",
    "        raise ValueError(f\"Required variable '{v}' not found.\")\n",
    "\n",
    "y_true = np.hstack([\n",
    "    np.ones(len(P_test_indices)),\n",
    "    np.zeros(len(U_test_indices))\n",
    "])\n",
    "\n",
    "test_indices = P_test_indices + U_test_indices\n",
    "\n",
    "model_order = list(final_predictions_by_threshold.keys())\n",
    "if \"RandomForest\" in model_order:\n",
    "    model_order.remove(\"RandomForest\")\n",
    "    model_order.append(\"RandomForest\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for model_name in model_order:\n",
    "\n",
    "    result = final_predictions_by_threshold[model_name]\n",
    "    y_score = result['weighted'][test_indices]\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    fpr_unique, unique_idx = np.unique(fpr, return_index=True)\n",
    "    tpr_unique = tpr[unique_idx]\n",
    "    fpr_dense = np.linspace(0, 1, 2000)\n",
    "    pchip = PchipInterpolator(fpr_unique, tpr_unique)\n",
    "    tpr_smooth = pchip(fpr_dense)\n",
    "    tpr_smooth = np.clip(tpr_smooth, 0, 1)\n",
    "    tpr_smooth[0] = 0.0\n",
    "    tpr_smooth[-1] = 1.0\n",
    "    if model_name == \"RandomForest\":\n",
    "        lw = 1\n",
    "        zo = 10\n",
    "    else:\n",
    "        lw = 1\n",
    "        zo = 2\n",
    "\n",
    "    plt.plot(\n",
    "        fpr_dense,\n",
    "        tpr_smooth,\n",
    "        linewidth=lw,\n",
    "        zorder=zo,\n",
    "        label=f\"{model_name} (AUC = {roc_auc:.3f})\"\n",
    "    )\n",
    "\n",
    "plt.plot(\n",
    "    [0, 1], [0, 1],\n",
    "    'k--',\n",
    "    linewidth=1.2,\n",
    "    label=\"Random\",\n",
    "    zorder=1\n",
    ")\n",
    "\n",
    "plt.xlabel(\"False Positive\", fontweight='bold')\n",
    "plt.ylabel(\"True Positive\", fontweight='bold')\n",
    "\n",
    "plt.legend(loc=\"lower right\", fontsize=12)\n",
    "plt.grid(alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\n",
    "    \"ROC.png\",\n",
    "    dpi=1200,\n",
    "    bbox_inches=\"tight\"\n",
    ")\n",
    "plt.close()\n",
    "\n",
    "print(\"✓ Saved: ROC.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
